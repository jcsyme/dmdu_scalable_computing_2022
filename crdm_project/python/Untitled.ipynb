{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'setup_analysis' has no attribute 'dict_init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-0879050909bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#get ptn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mptn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ptn\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m#get name of model runs for tagging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mtag_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_init\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"tag_name\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'setup_analysis' has no attribute 'dict_init'"
     ]
    }
   ],
   "source": [
    "\n",
    "#amazon aws\n",
    "import boto3\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, os.path\n",
    "import shutil\n",
    "import sys\n",
    "import base64\n",
    "import time\n",
    "import setup_analysis as sa\n",
    "\n",
    "##################################\n",
    "###                            ###\n",
    "###    START:INITIALIZATION    ###\n",
    "###                            ###\n",
    "##################################\n",
    "\n",
    "#############################\n",
    "#    INITIALIZATION FILE    #\n",
    "#############################\n",
    "\n",
    "#get ptn\n",
    "ptn = sa.dict_init[\"ptn\"]\n",
    "#get name of model runs for tagging\n",
    "tag_name = sa.dict_init[\"tag_name\"]\n",
    "#number of instnaces to launch\n",
    "n_instances = sa.dict_init[\"n_instances\"]\n",
    "#get launch parameter information\n",
    "ami = sa.dict_init[\"ami\"]\n",
    "#subnet\n",
    "subnet = sa.dict_init[\"subnet\"]\n",
    "#vpc\n",
    "vpc = sa.dict_init[\"vpc\"]\n",
    "#security group\n",
    "security_group = sa.dict_init[\"security_group\"]\n",
    "#instance type\n",
    "type_instance = sa.dict_init[\"instance_type\"]\n",
    "#iam role\n",
    "iam_role = sa.dict_init[\"iam_role\"]\n",
    "#s3 information\n",
    "s3_bucket = sa.dict_init[\"s3_bucket\"]\n",
    "s3_key_upload = sa.dict_init[\"s3_upload_key\"]\n",
    "s3_key_temporary = sa.dict_init[\"s3_temporary_key\"]\n",
    "s3_key_model = sa.dict_init[\"s3_storage_key\"]\n",
    "#date stamp\n",
    "session_stamp = str(sa.dict_init[\"analysis_run_id\"])\n",
    "\n",
    "\n",
    "gen_tiu_q = False\n",
    "###   LOAD SOME TABLES\n",
    "\n",
    "df_attribute_primary_id = pd.read_csv(sa.fp_csv_attribute_primary_id)\n",
    "df_experiment_primary_ids = pd.read_csv(sa.fp_csv_experiment_primary_ids)\n",
    "df_model_input_database = pd.read_csv(sa.fp_csv_model_input_database)\n",
    "primaries_to_run = list(set(df_experiment_primary_ids[sa.field_primary_key]))\n",
    "# reduce for speed\n",
    "df_model_input_database = df_model_input_database[df_model_input_database[sa.field_primary_key].isin(primaries_to_run)].sort_values(by = [sa.field_primary_key, \"year\", \"month\"]).reset_index(drop = True)\n",
    "\n",
    "###   PATH INITIALIZATIONS\n",
    "\n",
    "#set export directory\n",
    "dir_tmp_export = sr.dir_tmp\n",
    "#check existence\n",
    "if (os.path.exists(dir_tmp_export)) == True:\n",
    "    #clear out the directory\n",
    "    shutil.rmtree(dir_tmp_export)\n",
    "#make the directory anew\n",
    "os.makedirs(dir_tmp_export, exist_ok = True)\n",
    "\n",
    "#get experimental design file\n",
    "path_exp_design = sa.fp_csv_model_input_database\n",
    "path_exp_design = path_exp_design.replace(\"$$$dir_model$$$\", dir_model)\n",
    "#exists?\n",
    "if os.path.exists(path_exp_design) != True:\n",
    "    #notify an exit\n",
    "    print(\"Experimental design file \" + str(path_exp_design) + \" not found. Exiting...\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "\n",
    "#############################\n",
    "#    INITIALIZE AWS INFO    #\n",
    "#############################\n",
    "\n",
    "#initialize session\n",
    "b3s = boto3.Session(profile_name = \"default\")\n",
    "#start ec2 object\n",
    "ec2 = b3s.resource(\"ec2\")\n",
    "#start s3 object\n",
    "s3 = b3s.resource(\"s3\")\n",
    "#get client\n",
    "ec2client = boto3.client(\"ec2\")\n",
    "s3client = boto3.client(\"s3\")\n",
    "#start instance list\n",
    "instanceList = []\n",
    "\n",
    "###   CHECK S3\n",
    "\n",
    "#get all bucket available\n",
    "allBuckets = s3client.list_buckets()\n",
    "ab = []\n",
    "for a in allBuckets[\"Buckets\"]:\n",
    "    #get names\n",
    "    ab.append(a[\"Name\"])\n",
    "#buckets ok? (initialize as false)\n",
    "bucketsQ = False\n",
    "#check membership\n",
    "if s3_bucket in ab:\n",
    "    bucketsQ = True\n",
    "#exit if bucket is not found\n",
    "if bucketsQ != True:\n",
    "    print(\"Error: one or more buckets not found in s3.\")\n",
    "    sys.exit()\n",
    "\n",
    "\n",
    "#check for temporary bucket ahead of time; clear if it exists\n",
    "s3_key_exists = s3client.list_objects(Bucket = s3_bucket, Prefix = s3_key_temporary)\n",
    "#create bucket object\n",
    "bucket_check = s3.Bucket(s3_bucket)\n",
    "#clean up\n",
    "if s3_key_exists.get(\"Contents\") != None:\n",
    "    #notify of remove\n",
    "    print(\"(TEMP DISABLE FOR TESTING) Removing \" + s3_key_temporary + \" from \" + s3_bucket + \".\")\n",
    "else:\n",
    "    #notify that not found\n",
    "    print(s3_key_temporary + \" not found in bucket s3://\" + s3_bucket + \". It will be created.\")\n",
    "\n",
    "\n",
    "#set output bucket key\n",
    "s3_key_model_stamped = s3_key_model + \"/\" + session_stamp\n",
    "#check for model output bucket ahead of time; clear if it exists\n",
    "s3_key_exists2 = s3client.list_objects(Bucket = s3_bucket, Prefix = s3_key_model_stamped)\n",
    "#create bucket object\n",
    "bucket_check2 = s3.Bucket(s3_bucket)\n",
    "#clean up\n",
    "if s3_key_exists2.get(\"Contents\") != None:\n",
    "    #notify of remove\n",
    "    print(\"(TEMPORARILY DISABLED FOR TESTING) Removing \" + s3_key_model_stamped + \" from \" + s3_bucket + \".\")\n",
    "else:\n",
    "    #notify that not found\n",
    "    print(s3_key_model_stamped + \" not found in bucket s3://\" + s3_bucket + \". It will be created.\")\n",
    "\n",
    "\n",
    "\n",
    "###################################\n",
    "###                             ###\n",
    "###    UPLOAD REQUIRED DATA     ###\n",
    "###                             ###\n",
    "###################################\n",
    "\n",
    "\n",
    "if not gen_tiu_q:\n",
    "\n",
    "    ###   FIRST, CREATE INTEGRATED MODEL TARBALL\n",
    "\n",
    "    #set file path for system run and directory name of integrated model to upload\n",
    "    dirname_upload = \"model_upload\"\n",
    "    fn_mod_compressed = f\"{dirname_upload}.tar.gz\"\n",
    "    fp_mod_compressed = os.path.join(os.path.dirname(sa.dir_proj), fn_mod_compressed)\n",
    "\n",
    "\n",
    "\n",
    "    # COPY OVER INTEGRATED MODEL\n",
    "\n",
    "    #temporary directory\n",
    "    fp_tmp = os.path.join(os.path.dirname(sa.dir_proj), dirname_upload)\n",
    "    #notify\n",
    "    t0_copy = time.time()\n",
    "    print(\"Copying \" + sa.dir_proj + \" to  \" + fp_tmp + \"...\\n\")\n",
    "    #copy to tmp\n",
    "    comm_cp = \"rsync -a \\\"\" + dir_model + \"\\\" \\\"\" + fp_tmp + \"\\\"\"\n",
    "    #paths to exclude\n",
    "    vec_exclude_files_from_copy = vec_exclude_files_from_copy + [\n",
    "        sa.dir_out,\n",
    "        sa.fp_csv_experiment_primary_ids,\n",
    "        sa.fp_csv_model_input_database,\n",
    "        sa.fp_csv_model_output_database\n",
    "    ]\n",
    "    #ensure uniqueness\n",
    "    vec_exclude_files_from_copy = list(set(vec_exclude_files_from_copy))\n",
    "    #loop\n",
    "    for fp in vec_exclude_files_from_copy:\n",
    "        #string to replace\n",
    "        str_repl = dir_model + os.path.sep\n",
    "        #clean\n",
    "        fp_clean = fp.replace(str_repl, \"\")\n",
    "        #update command\n",
    "        comm_cp = comm_cp + \" --exclude \\\"\" + fp_clean + \"\\\"\"\n",
    "    #execute it\n",
    "    cp_res = os.system(comm_cp)\n",
    "    #create an empty \"out\" directory\n",
    "    os.makedirs(os.path.join(fp_tmp, \"out\"), exist_ok = True)\n",
    "    #notify\n",
    "    print(\"Copy complete. Time elapsed \" + str(time.time() - t0_copy) + \" seconds.\")\n",
    "\n",
    "\n",
    "    #check to see if compressed version of model exists\n",
    "    if os.path.exists(fp_mod_compressed):\n",
    "        #remove it if so\n",
    "        os.remove(fp_mod_compressed)\n",
    "    #set script execution to generate new tarballâ€”IMPORTANT:COPYFILE_DISABLE=1 *MUST* BE SET TO 1 TO ELIMINATE ERRONEOUS FILES IN THE EXTRACTION\n",
    "    print(\"Compressing \" + dirname_upload + \" to  \" + fp_mod_compressed + \"...\\n\\n\")\n",
    "\n",
    "    # switch to the correct directory\n",
    "    dir_setback = os.getcwd()   \n",
    "    os.chdir(os.path.dirname(sa.dir_proj))\n",
    "    comm_tar = \"COPYFILE_DISABLE=1 tar -cvzf \\\"\" + fp_mod_compressed + \"\\\" \" + dirname_upload\n",
    "    #run\n",
    "    os.system(comm_tar)\n",
    "    os.chdir(dir_setback)\n",
    "    #remove temporary copy\n",
    "    shutil.rmtree(fp_tmp)\n",
    "\n",
    "    #\n",
    "\n",
    "    ###   THEN, SET FILE PATHS FOR UPLOAD\n",
    "\n",
    "    ##  ENERGY TARBALL\n",
    "\n",
    "    #notify\n",
    "    print(\"Uploading \" + fp_mod_compressed + \" to \" + s3_key_temporary + \"...\\n\")\n",
    "    #add upload key\n",
    "    s3_object_mod = s3_key_temporary + \"/\" + fn_mod_compressed\n",
    "    #add as object\n",
    "    s3client.upload_file(fp_mod_compressed, s3_bucket, s3_object_mod)\n",
    "\n",
    "    #get initialization file (aws) and upload\n",
    "    print(\"Uploading \" + sa.fp_ini_aws + \" to \" + s3_key_temporary + \"...\\n\")\n",
    "    #add upload key\n",
    "    s3_object_init_aws = s3_key_temporary + \"/initialize_aws.ini\"\n",
    "    #add as object\n",
    "    s3client.upload_file(sa.fp_ini_aws, s3_bucket, s3_object_init_aws)\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "#    START:EXPERIMENTAL DESIGN SUBSETTING    #\n",
    "##############################################\n",
    "\n",
    "#get experimental design\n",
    "exp_design = pd.read_csv(sa.fp_csv_experiment_primary_ids)\n",
    "#reduce\n",
    "exp_design = pd.DataFrame(exp_design[sa.field_primary_key])\n",
    "#add run ids to loop over\n",
    "primaries_all = list(set(exp_design[sa.field_primary_key]))\n",
    "primaries_all.sort()\n",
    "#number of futures\n",
    "n_primaries = len(primaries_all)\n",
    "\n",
    "#subset the master attribute and get unique time.series/design combinations\n",
    "df_attribute_primary_id = df_attribute_primary_id[df_attribute_primary_id[\"Master.ID\"].isin(set(exp_design[\"Master.ID\"]))]\n",
    "df_attribute_primary_id = df_attribute_primary_id.sort_values(by = [\"Master.ID\"])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "##############################################\n",
    "###                                        ###\n",
    "###    BUILD AND UPLOAD INSTANCE FILES     ###\n",
    "###                                        ###\n",
    "##############################################\n",
    "\n",
    "#initialize dictionary of session files to upload\n",
    "dict_session_inis = {}\n",
    "#initialize dictionary mapping instance id to session id\n",
    "dict_inst_to_session = {}\n",
    "#initialize data frame for indexing futures to instance\n",
    "df_instance_indexing = []\n",
    "#initialize list of output designs\n",
    "designFilesOut = []\n",
    "#set instance output index\n",
    "inst_id = 0\n",
    "\n",
    "##  BUILD SESSION INITIALIZATION TEMPLATE BY SPLIT\n",
    "\n",
    "#notify\n",
    "print(\"Starting build of session initialization files...\\n\")\n",
    "\n",
    "with open(sa.fp_ini_session, 'r') as file_template_session_ini:\n",
    "    template_session_ini = file_template_session_ini.readlines()\n",
    "# template file name\n",
    "fn_ini_session_upload = \"initialize_session_upload.ini\"\n",
    "#dictionary of replacements\n",
    "dict_repls = {\n",
    "    \"cloud_run_q\": \"True\",\n",
    "    \"build_experimental_design_q\": \"False\"\n",
    "}\n",
    "#adjusted file\n",
    "file_session_ini = []\n",
    "#loop over ini\n",
    "for j in range(len(template_session_ini)):\n",
    "    line = template_session_ini[j]\n",
    "    #check over keys\n",
    "    for key in dict_repls.keys():\n",
    "        key_str = str(key)\n",
    "        #check line\n",
    "        overwrite_q = (key_str == line[0:min(len(key_str), len(line))])\n",
    "        #\n",
    "        if overwrite_q:\n",
    "            line = str(key) + \":\\t\" + str(dict_repls[key]) + \"\\n\"\n",
    "    file_session_ini.append(line)\n",
    "#write to output\n",
    "fp_si = os.path.join(sa.dir_tmp, fn_ini_session_upload)\n",
    "#write lines\n",
    "with open(fp_si, \"w\") as file_si_out:\n",
    "    file_si_out.writelines(file_session_ini)\n",
    "\n",
    "    ##  ADD TO S3\n",
    "\n",
    "    if not gen_tiu_q:\n",
    "        #build s3 upload key for file\n",
    "        temp_key = s3_key_temporary + \"/\" + fn_si\n",
    "        #notify\n",
    "        print(\"Uploading file \" + fn_si + \" to s3://\" + s3_bucket + \"/\" + temp_key + \"\\n\")\n",
    "        #upload\n",
    "        s3.meta.client.upload_file(fp_si, s3_bucket, temp_key)\n",
    "\n",
    "#notify\n",
    "print(\"Session initialization file generation complete.\\n\\n\")\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "#    START DIVISION OF EXPERIMENTAL DESIGN AND UPLOAD TO S3    #\n",
    "################################################################\n",
    "\n",
    "print(\"Chopping experimental design...\\n\")\n",
    "\n",
    "#get information\n",
    "n_base_scenarios_per_inst = math.floor(n_primaries/n_instances)\n",
    "n_extra_scenarios = n_primaries%n_base_scenarios_per_inst\n",
    "\n",
    "#initialize number of extra lines added to this dt's set\n",
    "nelAdded = 0\n",
    "#initialize \"task list\"\n",
    "l_task = primaries_all\n",
    "\n",
    "#build files\n",
    "while len(l_task) > 0:\n",
    "\n",
    "    ##  GET SCENARIOS\n",
    "\n",
    "    #get index\n",
    "    if nelAdded != n_extra_scenarios:\n",
    "        #extra future to add\n",
    "        q = 1\n",
    "        #update\n",
    "        nelAdded = nelAdded + 1\n",
    "    else:\n",
    "        q = 0\n",
    "    #number of lines to add\n",
    "    n_scenarios_extract = n_base_scenarios_per_inst + q\n",
    "    #remove from list\n",
    "    scenarios_cur = l_task[0:n_scenarios_extract]\n",
    "\n",
    "\n",
    "    ## SUBSET AND INDEX\n",
    "\n",
    "    #build temporary file for upload\n",
    "    temp = df_model_input_database[df_model_input_database[sa.field_primary_key].isin(scenarios_cur)][[sa.field_primary_key]]\n",
    "    #generate index of\n",
    "    dfo = list(set(scenarios_cur))\n",
    "    dfo.sort()\n",
    "    dfo = [[x, inst_id] for x in dfo]\n",
    "    dfo = pd.DataFrame(dfo, columns = [sa.field_primary_key, \"instance_id\"])\n",
    "    #add to master index\n",
    "    df_instance_indexing.append(dfo)\n",
    "\n",
    "    ## EXPORT TO CSV\n",
    "\n",
    "    #build file name\n",
    "    fnOut = \"instance_\" + str(inst_id) + \".csv\"\n",
    "    #build file path out name\n",
    "    tempPathOut = os.path.join(dir_tmp_export, fnOut)\n",
    "    #write output\n",
    "    temp.to_csv(tempPathOut, header = True, index = False)\n",
    "\n",
    "    ##  S3 UPLOADS\n",
    "\n",
    "    if not gen_tiu_q:\n",
    "        #build s3 upload key for file\n",
    "        temp_key = s3_key_temporary + \"/\" + fnOut\n",
    "        #notify\n",
    "        print(\"Uploading file \" + fnOut + \" to s3://\" + s3_bucket + \"/\" + temp_key)\n",
    "        #upload\n",
    "        s3.meta.client.upload_file(tempPathOut, s3_bucket, temp_key)\n",
    "    #extract\n",
    "    designFilesOut.append(temp)\n",
    "    #update instance index\n",
    "    inst_id += 1\n",
    "    #reduce the task list\n",
    "    if len(l_task) > n_scenarios_extract:\n",
    "        #reduce list\n",
    "        l_task = l_task[n_scenarios_extract:len(l_task)]\n",
    "    else:\n",
    "        l_task = []\n",
    "\n",
    "df_instance_indexing = pd.concat(df_instance_indexing, axis = 0)\n",
    "#check to ensure integer\n",
    "for field in list(df_instance_indexing.columns):\n",
    "    df_instance_indexing[field] = np.array(df_instance_indexing[field]).astype(int)\n",
    "#export indexed csv\n",
    "df_instance_indexing.to_csv(os.path.join(dir_tmp_export, \"scenario_instance_index.csv\"), index = False)\n",
    "\n",
    "print(\"Instance data upload complete.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import setup_analysis as sa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'setup_analysis' from '/Users/jsyme/Documents/PRGS/Classes/2021/RDM Tutorial/tutorial2021/code/crdm_project/python/setup_analysis.py'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'setup_analysis' has no attribute 'dict_init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-407adf6da779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdict_init\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: module 'setup_analysis' has no attribute 'dict_init'"
     ]
    }
   ],
   "source": [
    "sa.dict_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
