{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'class_5_func' from '/Users/jsyme/Documents/PRGS/Classes/2021/RDM Tutorial/tutorial2021/code/workbooks_weekly/class_5_func.py'>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import pandas dataframe\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "from itertools import product\n",
    "import multiprocessing as mp\n",
    "import class_5_func as cf \n",
    "import importlib\n",
    "importlib.reload(cf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in data frmes and initialize an output template\n",
    "df_output = pd.read_csv(\"class_3_dataframes_example_output.csv\")\n",
    "df_input = pd.read_csv(\"class_3_dataframes_example_input.csv\")\n",
    "\n",
    "# initialize an output directory if it doesn't exist (in current directory)\n",
    "dir_out = os.path.join(os.getcwd(), \"out\")\n",
    "if not os.path.exists(dir_out):\n",
    "    os.makedirs(dir_out, exist_ok = True)\n",
    "    \n",
    "# create an output file path template\n",
    "fp_out_par_template = os.path.join(dir_out, \"class_5_partemporary_csv-##PROC##.csv\")\n",
    "fp_out_par_final = os.path.join(dir_out, \"class_5_paraggregate_csv.csv\")\n",
    "fp_out_serial = os.path.join(dir_out, \"class_5_serial_csv.csv\")\n",
    "\n",
    "\n",
    "##  EXAMPLE: create a primary key id for all possible scenarios (assuming that future_id and strategy_id can take on any values between their min/max across both files)\n",
    "\n",
    "# initialize a dictionary\n",
    "dict_fld_ranges = {}\n",
    "# id fields from both \n",
    "fields_id = [x for x in list(set(df_input.columns) & set(df_output.columns)) if (\"_id\" in x)]\n",
    "\n",
    "for field in fields_id:\n",
    "    rng = list(range(\n",
    "        min(min(df_input[field]), min(df_input[field])),\n",
    "        max(max(df_input[field]), max(df_input[field])) + 1\n",
    "    ))\n",
    "    \n",
    "    dict_fld_ranges.update({field: rng})\n",
    "\n",
    "# generate all possible combinations and store in key dataframe\n",
    "df_key = list(product(*list(dict_fld_ranges.values())))\n",
    "df_key = pd.DataFrame(df_key, columns = list(dict_fld_ranges.keys()))\n",
    "df_key[\"primary_id\"] = range(len(df_key))\n",
    "\n",
    "\n",
    "# add this key to the input file; for fun, sort on it\n",
    "df_input = pd.merge(df_input, df_key, how = \"left\", on = list(set(df_key.columns) & set(df_input.columns)))\n",
    "df_input.sort_values(by = [\"primary_id\"], inplace = True)\n",
    "df_input.reset_index(drop = True, inplace = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 for primary_id = 0 complete.\n",
      "Iteration 1 for primary_id = 1 complete.\n",
      "Iteration 2 for primary_id = 3 complete.\n",
      "Iteration 3 for primary_id = 4 complete.\n",
      "Iteration 4 for primary_id = 5 complete.\n",
      "Iteration 5 for primary_id = 7 complete.\n",
      "Iteration 6 for primary_id = 8 complete.\n",
      "Iteration 7 for primary_id = 9 complete.\n",
      "Iteration 8 for primary_id = 11 complete.\n",
      "Iteration 9 for primary_id = 44 complete.\n",
      "Iteration 10 for primary_id = 45 complete.\n",
      "Iteration 11 for primary_id = 47 complete.\n",
      "Iteration 12 for primary_id = 48 complete.\n",
      "Iteration 13 for primary_id = 49 complete.\n",
      "Iteration 14 for primary_id = 51 complete.\n",
      "Iteration 15 for primary_id = 52 complete.\n",
      "Iteration 16 for primary_id = 53 complete.\n",
      "Iteration 17 for primary_id = 55 complete.\n",
      "Iteration 18 for primary_id = 56 complete.\n",
      "Iteration 19 for primary_id = 57 complete.\n",
      "Iteration 20 for primary_id = 59 complete.\n",
      "Time elapsed to create serial file: 88.19448399543762 seconds.\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "#    RUN SERIALLY    #\n",
    "######################\n",
    "\n",
    "t0_serial = time.time()\n",
    "# run serial example\n",
    "cf.example_do(df_input, fp_out_serial)\n",
    "t1_serial = time.time()\n",
    "\n",
    "t_elapse_serial = t1_serial - t0_serial\n",
    "\n",
    "print(\"Time elapsed to create serial file: %s seconds.\"%(t_elapse_serial))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time elapsed to create parallel files: 19.32193112373352 seconds.\n",
      "Total time elapsed including reaggregation of parallel files: 19.40291714668274 seconds.\n"
     ]
    }
   ],
   "source": [
    "###############################\n",
    "#    ASYNCHRONOUS PARALLEL    #\n",
    "###############################\n",
    "\n",
    "t0_par_async = time.time()\n",
    "\n",
    "#\n",
    "# SOLUTION TO GET APPLY_ASYNC TO WORK WITH JUPYTER LAB: FUNCTION HAS TO BE PLACED IN MODULE AND IMPORTED: https://stackoverflow.com/questions/47313732/jupyter-notebook-never-finishes-processing-using-multiprocessing-python-3\n",
    "# https://towardsdatascience.com/asynchronous-parallel-programming-in-python-with-multiprocessing-a3fc882b4023\n",
    "#\n",
    "\n",
    "# initialize output vector/array (pre-allocate memory)\n",
    "vec_collect_run_status = []\n",
    "\n",
    "# set up dummy functions to get results\n",
    "def get_result(result):\n",
    "    \n",
    "    global vec_collect_run_status\n",
    " \n",
    "    # update\n",
    "    vec_collect_run_status.append(result)\n",
    "\n",
    "\n",
    "# check to ensure current module is \"__main__\"; this is necessary in scripts that use multiprocessing. Without it, the processing framework will run the entirety of the original script in parallel\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # set the numbe of CPUs\n",
    "    n_cpus = 7#mp.cpu_count()\n",
    "    # find the number of iterations\n",
    "    n_rows_min = int(math.floor(len(df_input)/n_cpus))\n",
    "    n_rows_extra = len(df_input)%n_cpus\n",
    "    # start the MP pool for asynchronous parallelization\n",
    "    pool = mp.Pool(n_cpus)\n",
    "\n",
    "    # for ease, create a list to store output file paths in\n",
    "    fps_out = []\n",
    "    \n",
    "    j = 0\n",
    "    # apply the function; note: if the function only takes one argument (e.g., f(x)), make sure the args is args = (x, ) - that extra comma is important\n",
    "    for i in range(n_cpus):\n",
    "        \n",
    "        # get number of rows of the data frame to send to the process\n",
    "        if i <= n_rows_extra:\n",
    "            n_take = n_rows_min + 1\n",
    "        else:\n",
    "            n_take = n_rows_min\n",
    "        df_tmp = df_input[j:(j + n_take)]\n",
    "        \n",
    "        # update the row index\n",
    "        j = j + n_take\n",
    "        \n",
    "        # set the output file path\n",
    "        fp_out_cur = fp_out_template.replace(\"##PROC##\", str(i))\n",
    "        fps_out.append(fp_out_cur)\n",
    "        \n",
    "        pool.apply_async(\n",
    "            # target function\n",
    "            cf.example_do,\n",
    "            # function arguments \n",
    "            args = (df_tmp, fp_out_cur),\n",
    "            callback = get_result\n",
    "        )\n",
    "\n",
    "\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    t1_par_async = time.time()\n",
    "\n",
    "    # \n",
    "    t_elapse_par_async = t1_par_async - t0_par_async\n",
    "\n",
    "    print(\"Time elapsed to create parallel files: %s seconds.\"%(t_elapse_par_async))\n",
    "\n",
    "\n",
    "\n",
    "##  collect parallel files, write to single file\n",
    "\n",
    "init_q = True\n",
    "for fp in fps_out:\n",
    "    #create new file\n",
    "    df_tmp = pd.read_csv(fp)\n",
    "    \n",
    "    #write to new file\n",
    "    if init_q:\n",
    "        # write to output; explicit output options here \n",
    "        df_tmp.to_csv(fp_out_final, index = None, encoding = \"UTF-8\", mode = \"w\", header = True)\n",
    "        # get header\n",
    "        header_write = list(df_tmp.columns)\n",
    "        # turn off initialization\n",
    "        init_q = False\n",
    "    else:\n",
    "        # use mode = \"a\" to append; note that the header is stored in header_write\n",
    "        df_tmp[header_write].to_csv(fp_out_final, index = None, encoding = \"UTF-8\", mode = \"a\", header = False)\n",
    "\n",
    "t2_par_async = time.time()\n",
    "t_elapse_par_async_wreagg = t2_par_async - t0_par_async\n",
    "    \n",
    "print(\"Total time elapsed including reaggregation of parallel files: %s seconds.\"%(t_elapse_par_async_wreagg))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
